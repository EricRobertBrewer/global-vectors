{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'brown-europarl'\n",
    "filename = 'brown-europarl'\n",
    "is_text_already_split_on_sentences = True\n",
    "with open('corpora/{}/{}.txt'.format(directory, filename), 'r') as fd:\n",
    "    text = fd.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine whether or not to break the word window on sentence endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_break_context_window_on_sentences = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the raw text into sentences and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove punctuation, special characters.\n",
    "# TODO: !!! Keep apostrophe (') when in the middle of a word.\n",
    "text = re.sub('[\\',-=;:\"#+<>%$_()&*@^\\[\\]`~{}|\\\\\\\\]+', ' ', text)\n",
    "# Create sentences, or boundaries for our context window.\n",
    "if do_break_context_window_on_sentences:\n",
    "    if is_text_already_split_on_sentences:\n",
    "        # The text is split on sentence endings.\n",
    "        text = re.sub('[.?!]+', ' ', text)\n",
    "    else:\n",
    "        # Create sentence endings in a naive way.\n",
    "        text = re.sub('[.?!]+', '\\n', text)\n",
    "    sentences = text.split('\\n')\n",
    "else:\n",
    "    # Treat the entire text as a single sentence.\n",
    "    text = re.sub('[.?!]+', ' ', text)\n",
    "    sentences = [text]\n",
    "# TODO: Do we have to convert all of the text to lowercase?\n",
    "token_sentences = [[token.lower() for token in sentence.strip().split()] for sentence in sentences]\n",
    "print(\"Sentences: {}\".format(len(token_sentences)))\n",
    "token_count = len(token_sentences[0])\n",
    "vocab = set(token_sentences[0])\n",
    "for sentence in token_sentences[1:]:\n",
    "    token_count += len(sentence)\n",
    "    vocab.update(sentence)\n",
    "print(\"Tokens: {}\".format(token_count))\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "m = len(vocab)\n",
    "print(\"Vocabulary terms: {}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the vocabulary to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpora/{}/{}.vocab'.format(directory, filename), 'w') as fd_vocab:\n",
    "    for i in range(m):\n",
    "        fd_vocab.write(\"{}\\n\".format(vocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid using `vocab.index(word)` since it runs in O(n); instead, create a dictionary to look up word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {}\n",
    "for i, v in enumerate(vocab):\n",
    "    lookup[v] = i\n",
    "lookup['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the co-occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# r = Window radius: Terms to the left or right of a\n",
    "#  given term defined as being in the same \"context\".\n",
    "r = 8\n",
    "co = np.zeros((m, m))\n",
    "def occur(co, token, other):\n",
    "    \"\"\"\n",
    "    Update the co-occurrence matrix when a word appears in another word's context.\n",
    "    \"\"\"\n",
    "    i = lookup[token]\n",
    "    j = lookup[other]\n",
    "    co[i, j] += 1\n",
    "\n",
    "for sentence in token_sentences:\n",
    "    for t, token in enumerate(sentence):\n",
    "        # Count co-occurrences to the left of this term.\n",
    "        for other in sentence[max(0, t - r):t]:\n",
    "            occur(co, token, other)\n",
    "        # Count co-occurrences to the right of this term.\n",
    "        for other in sentence[t + 1:min(t + 1 + r, len(sentence))]:\n",
    "            occur(co, token, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent - _this is the slowest part!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = 200\n",
    "iterations = 25\n",
    "co_max = np.max(co)\n",
    "weight_alpha = 3/4\n",
    "learning_rate = .01\n",
    "\n",
    "center_embedding = np.random.uniform(low=-1, high=1, size=(m,vector_dim))\n",
    "context_embedding = np.random.uniform(low=-1, high=1, size=(m,vector_dim))\n",
    "\n",
    "center_bias = np.random.uniform(low=-1, high=1, size=(m))\n",
    "context_bias = np.random.uniform(low=-1, high=1, size=(m))\n",
    "\n",
    "##### Variable update historical arrays\n",
    "center_history = np.zeros((m,vector_dim)) + .1\n",
    "context_history = np.zeros((m, vector_dim)) + .1\n",
    "bias_center_history = np.zeros(m) + .1\n",
    "bias_context_history = np.zeros(m) + .1\n",
    "\n",
    "def weight_fun(x, co_max, alpha):\n",
    "    if x >= co_max:\n",
    "        return 1\n",
    "    return np.power(x/co_max, alpha)\n",
    "\n",
    "losses = []\n",
    "for iters in range(iterations):\n",
    "    global_loss = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            count = co[i,j]\n",
    "            if count == 0:\n",
    "                continue\n",
    "            center = center_embedding[i,:]\n",
    "            context = context_embedding[j,:]\n",
    "            b1 = center_bias[i]\n",
    "            b2 = context_bias[j]\n",
    "            weight = weight_fun(count, co_max, weight_alpha)\n",
    "            inner_loss = np.dot(center, context) + b1 + b2 - np.log(count)\n",
    "            loss = weight * np.square(inner_loss)\n",
    "            global_loss += loss\n",
    "\n",
    "            ### Compute Gradients\n",
    "            grad_center = weight * inner_loss * context\n",
    "            grad_context = weight * inner_loss * center\n",
    "            grad_bias_center = weight * inner_loss\n",
    "            grad_bias_context = weight * inner_loss\n",
    "\n",
    "            center_embedding[i,:] -=  learning_rate * (grad_center  / np.sqrt(center_history[i,:]))\n",
    "            context_embedding[j,:] -= learning_rate * (grad_context / np.sqrt(context_history[j,:]))\n",
    "            center_bias[i] -=  learning_rate * (grad_bias_center / np.sqrt(bias_center_history[i]))\n",
    "            context_bias[j] -= learning_rate * (grad_bias_context / np.sqrt(bias_context_history[j]))\n",
    "\n",
    "            center_history[i,:] += np.square(grad_center)\n",
    "            context_history[j,:] += np.square(grad_context)\n",
    "            bias_center_history[i] += np.square(grad_bias_center)\n",
    "            bias_context_history[j] += np.square(grad_bias_context)\n",
    "    losses.append(global_loss)\n",
    "    print(\"Completed iteration: {}\".format(iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Write the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"{}_{}_{}_{}.center\".format(filename, r, vector_dim, iterations), \"w\") as fd_center:\n",
    "    for i in range(m):\n",
    "        for j in range(vector_dim):\n",
    "            fd_center.write(\"{} \".format(center_embedding[i][j]))\n",
    "        # Write the bias weight.\n",
    "        fd_center.write(\"{}\\n\".format(center_bias[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"{}_{}_{}_{}.context\".format(filename, r, vector_dim, iterations), \"w\") as fd_context:\n",
    "    for i in range(m):\n",
    "        for j in range(vector_dim):\n",
    "            fd_context.write(\"{} \".format(context_embedding[i][j]))\n",
    "        # Write the bias weight.\n",
    "        fd_context.write(\"{}\\n\".format(context_bias[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
